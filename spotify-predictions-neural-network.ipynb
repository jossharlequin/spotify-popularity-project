{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\josep\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>explicit</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>popularity</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>duration_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.995</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.1950</td>\n",
       "      <td>f</td>\n",
       "      <td>0.563</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>-12.428</td>\n",
       "      <td>t</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>118.469</td>\n",
       "      <td>0.7790</td>\n",
       "      <td>158.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.994</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>f</td>\n",
       "      <td>0.901</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>-28.454</td>\n",
       "      <td>t</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>83.972</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>282.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.604</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>f</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1190</td>\n",
       "      <td>-19.924</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9290</td>\n",
       "      <td>107.177</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>104.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.995</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>f</td>\n",
       "      <td>0.887</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>-14.734</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>108.003</td>\n",
       "      <td>0.7200</td>\n",
       "      <td>180.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.990</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.2040</td>\n",
       "      <td>f</td>\n",
       "      <td>0.908</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0980</td>\n",
       "      <td>-16.829</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0424</td>\n",
       "      <td>62.149</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>687.733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acousticness  danceability  energy explicit  instrumentalness  key  \\\n",
       "0         0.995         0.708  0.1950        f             0.563   10   \n",
       "1         0.994         0.379  0.0135        f             0.901    8   \n",
       "2         0.604         0.749  0.2200        f             0.000    5   \n",
       "3         0.995         0.781  0.1300        f             0.887    1   \n",
       "4         0.990         0.210  0.2040        f             0.908   11   \n",
       "\n",
       "   liveness  loudness mode  popularity  speechiness    tempo  valence  \\\n",
       "0    0.1510   -12.428    t           0       0.0506  118.469   0.7790   \n",
       "1    0.0763   -28.454    t           0       0.0462   83.972   0.0767   \n",
       "2    0.1190   -19.924    f           0       0.9290  107.177   0.8800   \n",
       "3    0.1110   -14.734    f           0       0.0926  108.003   0.7200   \n",
       "4    0.0980   -16.829    t           1       0.0424   62.149   0.0693   \n",
       "\n",
       "   duration_seconds  \n",
       "0           158.648  \n",
       "1           282.133  \n",
       "2           104.300  \n",
       "3           180.760  \n",
       "4           687.733  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the spotify data in from data.csv\n",
    "url = 'https://media.githubusercontent.com/media/jossharlequin/spotify-popularity-project/main/Resources/sql_spotify_data.csv'\n",
    "spotify_df = pd.read_csv(url)\n",
    "spotify_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting popularity as the target variable and setting the remaining columns as features\n",
    "y = spotify_df.popularity.values\n",
    "X = spotify_df.drop(columns='popularity').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Fitting the X data\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m X_scaler \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Scaling the X data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m X_scaler\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m    860\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 861\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    868\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\josep\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'f'"
     ]
    }
   ],
   "source": [
    "# Scaling the data using StandarScaler as a preprocessing step for the neural network\n",
    "# Creating the StandardScalar instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting the X data\n",
    "X_scaler = scaler.fit(X)\n",
    "\n",
    "# Scaling the X data\n",
    "X_scaled = X_scaler.transform(X)\n",
    "\n",
    "# Splitting training/test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josep\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Binning the target variable into 10 groups  \n",
    "bins = [-1, 20, 40, 60, 80, 100]\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "y_train_binned = pd.cut(y_train, bins=bins, labels=labels)\n",
    "y_test_binned = pd.cut(y_test, bins=bins, labels=labels)\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_one_hot = encoder.fit_transform(y_train_binned.reshape(-1,1))\n",
    "y_test_one_hot = encoder.transform(y_test_binned.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 1.0009 - accuracy: 0.5714\n",
      "Epoch 2/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.9323 - accuracy: 0.6010\n",
      "Epoch 3/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.9242 - accuracy: 0.6039\n",
      "Epoch 4/10\n",
      "3983/3983 [==============================] - 9s 2ms/step - loss: 0.9196 - accuracy: 0.6047\n",
      "Epoch 5/10\n",
      "3983/3983 [==============================] - 9s 2ms/step - loss: 0.9171 - accuracy: 0.6061\n",
      "Epoch 6/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.9149 - accuracy: 0.6061\n",
      "Epoch 7/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.9133 - accuracy: 0.6072\n",
      "Epoch 8/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.9120 - accuracy: 0.6063\n",
      "Epoch 9/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.9109 - accuracy: 0.6060\n",
      "Epoch 10/10\n",
      "3983/3983 [==============================] - 7s 2ms/step - loss: 0.9102 - accuracy: 0.6073\n",
      "1328/1328 - 2s - loss: 0.9128 - accuracy: 0.6057 - 2s/epoch - 2ms/step\n",
      "Loss: 0.9128164649009705, Accuracy: 0.6056547164916992\n"
     ]
    }
   ],
   "source": [
    "# Defining the neural network\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=16, activation=\"relu\", input_dim=13))\n",
    "nn_model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n",
    "\n",
    "# Compile the sequential model together and customize metrics\n",
    "nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model\n",
    "fit_model = nn_model.fit(X_train, y_train_one_hot, epochs=10)\n",
    "\n",
    "# Evaluating the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test, y_test_one_hot, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3983/3983 [==============================] - 9s 2ms/step - loss: 0.9962 - accuracy: 0.5791\n",
      "Epoch 2/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.9376 - accuracy: 0.5999\n",
      "Epoch 3/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.9275 - accuracy: 0.6038\n",
      "Epoch 4/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.9216 - accuracy: 0.6046\n",
      "Epoch 5/10\n",
      "3983/3983 [==============================] - 7s 2ms/step - loss: 0.9179 - accuracy: 0.6059\n",
      "Epoch 6/10\n",
      " 926/3983 [=====>........................] - ETA: 5s - loss: 0.9100 - accuracy: 0.6076"
     ]
    }
   ],
   "source": [
    "# Defining the neural network\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=16, activation=\"tanh\", input_dim=13))\n",
    "nn_model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n",
    "\n",
    "# Compile the sequential model together and customize metrics\n",
    "nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model\n",
    "fit_model = nn_model.fit(X_train, y_train_one_hot, epochs=10)\n",
    "\n",
    "# Evaluating the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test, y_test_one_hot, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy score: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\josep\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\josep\\anaconda3\\envs\\dev\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3983/3983 [==============================] - 10s 2ms/step - loss: 0.9379 - accuracy: 0.5945\n",
      "Epoch 2/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.9019 - accuracy: 0.6084\n",
      "Epoch 3/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.8941 - accuracy: 0.6102\n",
      "Epoch 4/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.8893 - accuracy: 0.6126\n",
      "Epoch 5/10\n",
      "3983/3983 [==============================] - 9s 2ms/step - loss: 0.8854 - accuracy: 0.6144\n",
      "Epoch 6/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.8830 - accuracy: 0.6148\n",
      "Epoch 7/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.8805 - accuracy: 0.6161\n",
      "Epoch 8/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.8786 - accuracy: 0.6162\n",
      "Epoch 9/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.8770 - accuracy: 0.6166\n",
      "Epoch 10/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.8756 - accuracy: 0.6178\n",
      "1328/1328 - 2s - loss: 0.8886 - accuracy: 0.6150 - 2s/epoch - 2ms/step\n",
      "Loss: 0.8885927796363831, Accuracy: 0.615047812461853\n"
     ]
    }
   ],
   "source": [
    "# Defining the neural network\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=50, activation=\"relu\", input_dim=13))\n",
    "nn_model.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=50, activation=\"relu\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=5, activation=\"softmax\"))\n",
    "\n",
    "# Compile the sequential model together and customize metrics\n",
    "nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model\n",
    "fit_model = nn_model.fit(X_train, y_train_one_hot, epochs=10)\n",
    "\n",
    "# Evaluating the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test, y_test_one_hot, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3983/3983 [==============================] - 9s 2ms/step - loss: 0.9417 - accuracy: 0.5971\n",
      "Epoch 2/10\n",
      "3983/3983 [==============================] - 12s 3ms/step - loss: 0.9087 - accuracy: 0.6063\n",
      "Epoch 3/10\n",
      "3983/3983 [==============================] - 14s 4ms/step - loss: 0.8997 - accuracy: 0.6090\n",
      "Epoch 4/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.8936 - accuracy: 0.6113\n",
      "Epoch 5/10\n",
      "3983/3983 [==============================] - 9s 2ms/step - loss: 0.8903 - accuracy: 0.6133\n",
      "Epoch 6/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.8876 - accuracy: 0.6132\n",
      "Epoch 7/10\n",
      "3983/3983 [==============================] - 9s 2ms/step - loss: 0.8850 - accuracy: 0.6156\n",
      "Epoch 8/10\n",
      "3983/3983 [==============================] - 9s 2ms/step - loss: 0.8829 - accuracy: 0.6150\n",
      "Epoch 9/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.8809 - accuracy: 0.6153\n",
      "Epoch 10/10\n",
      "3983/3983 [==============================] - 8s 2ms/step - loss: 0.8794 - accuracy: 0.6169\n",
      "1328/1328 - 2s - loss: 0.8876 - accuracy: 0.6161 - 2s/epoch - 1ms/step\n",
      "Loss: 0.8876000046730042, Accuracy: 0.6160600781440735\n"
     ]
    }
   ],
   "source": [
    "# Defining the neural network\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "nn_model.add(tf.keras.layers.Dense(units=50, activation=\"tanh\", input_dim=13))\n",
    "nn_model.add(tf.keras.layers.Dense(units=50, activation=\"tanh\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=50, activation=\"tanh\"))\n",
    "nn_model.add(tf.keras.layers.Dense(units=5, activation=\"softmax\"))\n",
    "\n",
    "# Compile the sequential model together and customize metrics\n",
    "nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Training the model\n",
    "fit_model = nn_model.fit(X_train, y_train_one_hot, epochs=10)\n",
    "\n",
    "# Evaluating the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test, y_test_one_hot, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
